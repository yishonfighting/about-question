
### 背景
Kubernetes的采用量增长了数倍。很明显，Kubernetes是容器编排的不二选择。

与此同时，Prometheus也被认为是监控容器化和非容器化工作负载的绝佳选择。监控是任何基础设施的一个重要关注点，我们应该确保我们的监控设置具有高可用性和高可扩展性，以满足不断增长的基础设施的需求，特别是在采用Kubernetes的情况下。



显而易见，在部署全球平台的时候，会遇到下面的问题：

prometheus存在单点问题

prometheus单机存储和抓取能力都有上限，容易单点故障

不同地区各自部署监控的话，各个grafana面板的配置会很头疼

虽然有一种方式是通过部署N个prometheus分别抓取不同的target来分摊压力的，但是grafana就要为不同的图表配置不同的prometheus地址，复杂程度比较棘手



再补充展望一些问题，随着我们的集群规模越来越大，监控数据的种类和数量也越来越多，除了解决上面的高可用问题，我们还希望基于 prometheus 构建全局视图，主要需求有：

长期存储：1 个月以上的数据存储，希望存储的维护成本足够小，有容灾和迁移。（可以使用 influxdb，但influxdb没有现成的集群方案，且需要人力维护。最好是存放在云上的 tsdb 或者对象存储、文件存储上。）
无限拓展：单机prometheus无法满足，且为了隔离性，最好按功能做 shard，如 master 组件性能监控与 pod 资源等业务监控分开、主机监控与日志监控也分开。或者按租户、业务类型分开（实时业务、离线业务）。
全局视图：按类型分开之后，虽然数据分散了，但监控视图需要整合在一起，一个 grafana 里 n个面板就可以看到所有地域+集群+pod 的监控数据，操作更方便，不用多个 grafana 切来切去，或者 grafana中多个 datasource 切来切去。
无侵入性：不要对已有的 prometheus 做过多的修改，因为 prometheus 是开源项目，版本也在快速迭代，不能对 prometheus 本身代码做修改，最好做封装，对最上层用户透明。


### 方案拟定
对于此，prometheus官方的高可用有几种方案：

HA：即两套 prometheus 采集完全一样的数据，外边挂负载均衡
HA + 远程存储：除了基础的多副本prometheus，还通过Remote write 写入到远程存储，解决存储持久化问题
联邦集群：即federation，按照功能进行分区，不同的 shard 采集不同的数据，由Global节点来统一存放，解决监控数据规模的问题。


使用官方建议的多副本 + 集群仍然会遇到一些问题，本质原因是prometheus的本地存储没有数据同步能力，要在保证可用性的前提下再保持数据一致性是比较困难的，基本的多副本 proxy 满足不了要求，比如：

prometheus集群的后端有 A 和 B 两个实例，A 和 B 之间没有数据同步。A 宕机一段时间，丢失了一部分数据，如果负载均衡正常轮询，请求打到A 上时，数据就会异常。
如果 A 和 B 的启动时间不同，时钟不同，那么采集同样的数据时间戳也不同，就多副本的数据不相同
就算用了远程存储，A 和 B 不能推送到同一个 tsdb，如果每人推送自己的 tsdb，数据查询走哪边就是问题
官方建议数据做Shard，然后通过federation来实现高可用，但是边缘节点和Global节点依然是单点，需要自行决定是否每一层都要使用双节点重复采集进行保活。也就是仍然会有单机瓶颈。
另外部分敏感报警尽量不要通过global节点触发，毕竟从Shard节点到Global节点传输链路的稳定性会影响数据到达的效率，进而导致报警实效降低


目前大多数的 prometheus 的集群方案是在存储、查询两个角度上保证数据的一致:

存储角度：如果使用 remote write 远程存储， A 和 B后面可以都加一个 adapter，adapter做选主逻辑，只有一份数据能推送到 tsdb，这样可以保证一个异常，另一个也能推送成功，数据不丢，同时远程存储只有一份，是共享数据。方案可以参考这篇文章
存储角度：仍然使用 remote write 远程存储，但是 A 和 B 分别写入 tsdb1 和 tsdb2 两个时序数据库，利用sync的方式在 tsdb1 和2 之前做数据同步，保证数据是全量的。
查询角度：上边的方案需要自己实现，有侵入性且有一定风险，因此大多数开源方案是在查询层面做文章，比如thanos 或者victoriametrics，仍然是两份数据，但是查询时做数据去重和join。只是 thanos是通过 sidecar 把数据放在对象存储，victoriametrics是把数据remote write 到自己的 server 实例，但查询层 thanos-query 和victor的 promxy的逻辑基本一致，都是为全局视图服务

### 目前处理方案
采用prometheus  + thanos ，thanos 只是监控套件，可参考另外一个篇幅讲解对不同方案的选型。

先上架构图：

手游技术部 > 全球平台高可用监控搭建 > image2022-3-11_14-18-15.png

全球平台使用了thanos的默认模式 ： sidercar ，当然也可有receiver的模式（选型补充说明）



1，thanos querier组件可以反向代理到N个prometheus，然后grafana直接指向thanos querier即可，thanos querier会从N个prometheus同时查询数据，返回满足Promql的数据结果；

2，如果N个prometheus抓取的数据存在重复的，那么thanos querier会根据label自动去重，确保返回给grafana的数据不会重复，这个特性很关键。

